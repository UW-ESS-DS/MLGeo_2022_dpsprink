{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering - Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is an unsupervised problem because the goal is to discover structure on the basis of data sets. Clustering looks to find homogeneous subgroups among the observations. \n",
    "\n",
    "K-means is an unsupervised clustering method. The main idea is to separate the data points into K dinstinct clusters. We then have two problems to solve. First, we need to find the k centroids of the k clusters. Then, we need to affect each data point to the cluster which centroid is the closest to the data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is the following:\n",
    "\n",
    "* Choose the number k of clusters\n",
    "* Initialize the clusters and calculate the feature mean as initial the clusters centroid\n",
    "* Iterate until convergence (e.g. cluster assignemnt stops changing):\n",
    "  - Assign each data point to its corresponding cluster ID\n",
    "  - Compute the value of the objective function (sum ove the clusters of the distances between the points and the cluster centroids)\n",
    "  - Update the centers of the clusters\n",
    "The solution will converge to a local minimum. Consequently, the result depends on the initial stage. It is prudent to perform the algorithm several times with different initial configurations.\n",
    "  \n",
    "The results depends on the initial step because only local min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import useful Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import wget\n",
    "from math import cos, sin, pi, sqrt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow cytometer data is organized by files. \n",
    "The file has attributes about each particle, such as normalized scatter, red, orange, and green. These are measurements from the instrument iteself from light scattering.\n",
    "Other parameters, like diam (diameter) and Qc (carbon quota) are estimated from the light scatter measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "cc=wget.download(\"https://www.dropbox.com/s/dwa82x6xhjkhyw8/ug3_FCM_distribution.feather?dl=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is some underway data collected from a cruise in 2019\n",
    "\n",
    "underway_g3 = pd.read_feather(\"ug3_FCM_distribution.feather\")\n",
    "print(underway_g3.columns)\n",
    "underway_g3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(pd.unique(underway_g3['filename']))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file is 1 sample taken at a certain time, location, and depth. There are also replicates, or even triplicates, run on the same spatiotemporal scale to get uncertainty estimations on the instrument. We can either ignore the replicates/triplicates or take the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = underway_g3[underway_g3['filename']==files[1]]\n",
    "px.scatter(test1, 'norm.scatter', 'norm.red', color='pop',log_x=True, log_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(test1, 'norm.scatter','norm.orange',color='pop',log_x=True,log_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, populations are gated based on their log-transformed normalized scatter (x) and red (y). Orange flouresence can be used in addition to gate the synecho population. We can see there are 3 populations here, and these were previously gated from the parameters above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "df = test1[['norm.scatter','norm.red','norm.orange','norm.green','depth']]\n",
    "df['norm.scatter'] = np.log10(df['norm.scatter'])\n",
    "df['norm.red'] = np.log10(df['norm.red'])\n",
    "df['norm.orange'] = np.log10(df['norm.orange'])\n",
    "sns.pairplot(df, hue=\"norm.green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\")\n",
    "df = test1[['norm.scatter','norm.red','norm.orange','pop']]\n",
    "df['norm.scatter'] = np.log10(df['norm.scatter'])\n",
    "df['norm.red'] = np.log10(df['norm.red'])\n",
    "df['norm.orange'] = np.log10(df['norm.orange'])\n",
    "sns.pairplot(df, hue=\"pop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering\n",
    "\n",
    "The goal is to partition $n$ data points into $k$ clusters. Each observation is labeled to a cluster with the nearest mean.\n",
    "\n",
    "K-means is iterative:\n",
    "1. assume initial values for the mean of each of the $k$ clusters\n",
    "2. Compute the distance for each observation to each of the $k$ means\n",
    "3. Label each observation as belonging to the nearest means\n",
    "4. Find the *center of mass* (mean) of each group of labeled points. These are new means to step 1.\n",
    "\n",
    "\n",
    "In the following, we denote $n$ the number of data points, and $p$ the number of features for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "p = 3 #()\n",
    "print('We have {:d} data points, and each one has {:d} features'.format(n, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a numpy array with the 3 features and all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros(shape=(n,p))\n",
    "data[:,0] = df['norm.scatter']\n",
    "data[:,1] = df['norm.red']\n",
    "data[:,2] = df['norm.orange']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a function to initialize the centroid of the clusters. We choose random points within the range of values taken by the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_centers(data, k):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Initialize centroids\n",
    "    centers = np.zeros((k, np.shape(data)[1]))\n",
    "    # Loop on k centers\n",
    "    for i in range(0, k):\n",
    "        # Generate p random values between 0 and 1\n",
    "        dist = np.random.uniform(size=np.shape(data)[1])\n",
    "        # Use the random values to generate a point within the range of values taken by the data\n",
    "        centers[i, :] = np.min(data, axis=0) + (np.max(data, axis=0) - np.min(data, axis=0)) * dist\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to affect each data point to the closest centroid, we need to define the distance between two data points. The most common distance is the **Euclidean distance**:\n",
    "\n",
    "$d(x,y) = \\sqrt{\\sum_{i = 1}^p (x_i - y_i)^2}$\n",
    "\n",
    "where $x$ and $y$ are two data observation points with $p$ variables. There exist many other distance metrics, which can be called from the ``DistanceMetric`` sci-kit learn module. You can find a review here: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n",
    "\n",
    "We then define a function to compute the distance between each data point and each centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(data, centers, k):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Initialize distance\n",
    "    distance = np.zeros((np.shape(data)[0], k))\n",
    "    # Loop on n data points\n",
    "    for i in range(0, np.shape(data)[0]):\n",
    "        # Loop on k centroids\n",
    "        for j in range(0, k):\n",
    "            # Compute distance\n",
    "            distance[i, j] = sqrt(np.sum(np.square(data[i, :] - centers[j, :])))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function to affect each data point to the cluster which centroid is the closest to the point. We also define an objective function that will be minimized until we reach convergence.\n",
    "\n",
    "Our objective is to minimize the sum of the square of the distance between each point and the closest centroid:\n",
    "\n",
    "$obj = \\sum_{j = 1}^k \\sum_{i = 1}^{N_j} d(x^{(i)} , x^{(j)}) ^2$\n",
    "\n",
    "where $x^{(i)}$ is the $i^{th}$ point in the cluster $j$, $x^{(j)}$ is the centroid of the cluster $j$, and $N_j$ is the number of points in the cluster $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_objective(distance, clusters):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Initialize objective\n",
    "    objective = 0.0\n",
    "    # Loop on n data points\n",
    "    for i in range(0, np.shape(distance)[0]):\n",
    "        # Add distance to the closest centroid\n",
    "        objective = objective + distance[i, int(clusters[i])] ** 2.0\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clusters(distance):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Initialize clusters\n",
    "    clusters = np.zeros(np.shape(distance)[0])\n",
    "    # Loop on n data points\n",
    "    for i in range(0, np.shape(distance)[0]):\n",
    "        # Find closest centroid\n",
    "        best = np.argmin(distance[i, :])\n",
    "        # Assign data point to corresponding cluster\n",
    "        clusters[i] = best\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all points are assigned to a cluster, compute the new location of the centroid. It is just the value of the mean of all the points affected to that cluster:\n",
    "\n",
    "For $1 \\leq j \\leq k$, $x_p^{(j)} = \\frac{1}{N_j} \\sum_{i = 1}^{N_j} x_p^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centers(data, clusters, k):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Initialize centroids\n",
    "    centers = np.zeros((k, np.shape(data)[1]))\n",
    "    # Loop on clusters\n",
    "    for i in range(0, k):\n",
    "        # Select all data points in this cluster\n",
    "        subdata = data[clusters == i, :]\n",
    "        # If no data point in this cluster, generate randomly a new centroid\n",
    "        if (np.shape(subdata)[0] == 0):\n",
    "            centers[i, :] = init_centers(data, 1)\n",
    "        else:\n",
    "            # Compute the mean location of all data points in this cluster\n",
    "            centers[i, :] = np.mean(subdata, axis=0)\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now code the K-means algorithm by assembling all these functions. We stop the computation when the objective function no longer decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(data, k):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Initialize centroids\n",
    "    centers = init_centers(data, k)\n",
    "    # Initialize objective function to maximum square of distance between to data points time number od data points\n",
    "    objective_old = np.shape(data)[0] * np.sum(np.square(np.max(data, axis=0) - np.min(data, axis=0)))\n",
    "    # Initialize clusters\n",
    "    clusters_old = np.zeros(np.shape(data)[0])\n",
    "    # Start loop until convergence\n",
    "    stop_alg = False\n",
    "    while stop_alg == False:\n",
    "        # Compute distance between data points and centroids\n",
    "        distance = compute_distance(data, centers, k)\n",
    "        # Get new clusters\n",
    "        clusters_new = compute_clusters(distance)\n",
    "        # get new value of objective function\n",
    "        objective_new = compute_objective(distance, clusters_new)\n",
    "        # If objective function stops decreasing, end loop\n",
    "        if objective_new >= objective_old:\n",
    "            return (clusters_old, objective_old, centers)\n",
    "        else:\n",
    "            # Update the locations of the centroids\n",
    "            centers = compute_centers(data, clusters_new, k)\n",
    "            objective_old = objective_new\n",
    "            clusters_old = clusters_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run K-means with 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "(clusters, objective, centers) = kmeans(data, k)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(data[:, 1], data[:, 0], -data[:, 2], c=clusters)\n",
    "ax.scatter(centers[:, 1], centers[:, 0], -centers[:, 2], marker='o', s=300, c='black')\n",
    "ax.set_xlabel('norm.scatterer')\n",
    "ax.set_ylabel('norm.red')\n",
    "ax.set_ylabel('norm.orange')\n",
    "plt.title('Clusters for cytometer data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of variations is higher for norm.scatterer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result is very dependent on the choice of the initial centroids. Run next cells several times to see the variations of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = data.mean(axis=0)\n",
    "s = data.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(clusters, objective, centers) = kmeans(data, k)\n",
    "centers_norm = centers * s + m\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(data[:, 1], data[:, 2], -data[:, 0], c=clusters)\n",
    "ax.scatter(centers_norm[:, 1], centers_norm[:, 2], -centers_norm[:, 0], marker='o', s=300, c='black')\n",
    "ax.set_xlabel('norm.scatterer')\n",
    "ax.set_ylabel('norm.red')\n",
    "ax.set_ylabel('norm.orange')\n",
    "plt.title('Clusters for cytometer data')\n",
    "print(centers_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is very sensitive to the location of the initial centroid. Repeat the clustering N times and choose the clustering with the best objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kmeans(data, k, N):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    objective = np.zeros(N)\n",
    "    clusters = np.zeros((N, np.shape(data)[0]))\n",
    "    centers = np.zeros((N, k, np.shape(data)[1]))\n",
    "    # Run K-means N times\n",
    "    for i in range(0, N):\n",
    "        result = kmeans(data, k)\n",
    "        clusters[i, :] = result[0]\n",
    "        objective[i] = result[1]\n",
    "        centers[i, :, :] = result[2]\n",
    "    # Choose the clustering with the best value of the objective function\n",
    "    best = np.argmin(objective)\n",
    "    return (clusters[best, :], objective[best], centers[best, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat K-means 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "(clusters, objective, centers) = repeat_kmeans(data, k, N)\n",
    "centers_norm = centers * s + m\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(data[:, 1], data[:, 2], -data[:, 0], c=clusters)\n",
    "ax.scatter(centers_norm[:, 1], centers_norm[:, 2], -centers_norm[:, 0], marker='o', s=300, c='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means with Scikit learn\n",
    "\n",
    "We will now use [scikit learn](!https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#) toolbox to run kmeans. Follow that tutorial and apply it to your problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = data\n",
    "Y = np.asarray(df['pop'])\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(data)\n",
    "kmeans.labels_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore different pipeline\n",
    "\n",
    "\n",
    "estimators = [\n",
    "    (\"k_means_cyto_8\", KMeans(n_clusters=8)),\n",
    "    (\"k_means_cyto_3\", KMeans(n_clusters=3)),\n",
    "    (\"k_means_cyto_bad_init\", KMeans(n_clusters=3, n_init=1, init=\"random\")),\n",
    "]\n",
    "\n",
    "name,est=estimators[0]\n",
    "est.fit(X)\n",
    "labels = est.labels_\n",
    "\n",
    "fig = plt.figure(0, figsize=(4, 3))\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=labels.astype(float), edgecolor=\"k\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel(\"norm.scatter\")\n",
    "ax.set_ylabel(\"norm.red\")\n",
    "ax.set_zlabel(\"norm.orange\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of number of clusters: The Elbow Method\n",
    "\n",
    "The elbow method is designed to find the optimal number of clusters. It consists in performing the clustering algorithm with an increasing number of clusters $k$ and select and measuring the average distance between data points and the cluster centroids. There are two typical metrics in the Elbow method\n",
    "\n",
    "* **Distortion**: It is calculated as the average of the squared distances from the cluster centers of the respective clusters. Typically, the Euclidean distance metric is used.\n",
    "* **Inertia**: It is the sum of squared distances of samples to their closest cluster center.\n",
    "\n",
    "For each value of $k$, we compute the mean of the square of the distance between the data points and the centroid of the cluster to which they belong. We then plot this value as a function of $k$. Hopefully, it decreases and then reaches a plateau. The optimal number of clusters is the value for which it attains the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use a different dataset to illustrate the elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithful = pd.read_csv('faithful.csv')\n",
    "plt.plot(faithful.current, faithful.next, 'ko')\n",
    "plt.xlabel('Eruption time in minutes')\n",
    "plt.ylabel('Waiting time to next eruption')\n",
    "plt.title('Old Faithful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faithful = faithful.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elbow(data, clusters, centers, k):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    E = 0\n",
    "    for i in range(0, k):\n",
    "        distance = compute_distance(data[clusters == i, :], centers[i, :].reshape(1, -1), 1)\n",
    "        E = E + np.mean(np.square(distance))\n",
    "    return E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the value of E for different values of the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.zeros(8)\n",
    "for k in range(1, 9):\n",
    "    (clusters, objective, centers) = kmeans_pp(data_faithful, k)\n",
    "    E[k - 1] = compute_elbow(data_faithful, clusters, centers, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot $E$ as a function of $k$ and see where reaches a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, 9), E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow method does not always work very well. For example, see what happens when the points get closer to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "origin = np.array([3, 3])\n",
    "data_shrink = origin + alpha * np.sign(data_faithful - origin) * np.power(np.abs(data_faithful - origin), 2.0)\n",
    "plt.plot(data_shrink[:, 0], data_shrink[:, 1], 'ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.zeros(8)\n",
    "for k in range(1, 9):\n",
    "    (clusters, objective, centers) = kmeans_pp(data_shrink, k)\n",
    "    E[k - 1] = compute_elbow(data_shrink, clusters, centers, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, 9), E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what happens when we decrease the number of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "indices = np.random.uniform(size=np.shape(data_faithful)[0])\n",
    "subdata = data_faithful[indices < alpha, :]\n",
    "plt.plot(subdata[:, 0], subdata[:, 1], 'ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.zeros(8)\n",
    "for k in range(1, 9):\n",
    "    (clusters, objective, centers) = kmeans_pp(subdata, k)\n",
    "    E[k - 1] = compute_elbow(subdata, clusters, centers, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, 9), E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA before clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate synthetics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = np.array([[2, 2], [2, 8], [4, 3]])\n",
    "radius = [0.1, 1]\n",
    "synthetics = np.empty([0, 2])\n",
    "for i in range(0, 3):\n",
    "    X = centers[i, 0] + radius[0] * np.random.randn(100)\n",
    "    Y = centers[i, 1] + radius[1] * np.random.randn(100)\n",
    "    U = (X + Y) * sqrt(2) / 2\n",
    "    V = (X - Y) * sqrt(2) / 2\n",
    "    synthetics = np.concatenate([synthetics, np.vstack((U, V)).T])\n",
    "plt.plot(synthetics[:,0], synthetics[:,1], 'ko')\n",
    "plt.xlim(1, 9)\n",
    "plt.ylim(-6, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now do k-means clustering with 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "(clusters, objective, centers) = kmeans_pp(synthetics, 3)\n",
    "plt.scatter(synthetics[:,0], synthetics[:,1], c=clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we apply PCA + normalization before the clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "synthetics_pca = pca.fit_transform(synthetics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(synthetics_pca)\n",
    "synthetics_scaled = scaler.transform(synthetics_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(clusters, objective, centers) = kmeans_pp(synthetics_scaled, 3)\n",
    "plt.scatter(synthetics[:,0], synthetics[:,1], c=clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now retrieve the initial three clusters. Let us see what happens if we apply the same technique to the PNSN catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = catalog.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(data_pca)\n",
    "data_scaled = scaler.transform(data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(clusters, objective, centers) = kmeans_pp(data_scaled, 4)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(data[:, 1], data[:, 0], -data[:, 2], c=clusters)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_zlabel('Depth (km)')\n",
    "plt.title('Clusters for PNSN catalog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "\n",
    "cc=wget.download(\"https://www.dropbox.com/s/dwa82x6xhjkhyw8/ug3_FCM_distribution.feather?dl=1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f625eed87f201675869c1975f26c79747f846dd12cd9c70305bdb23b2c204f1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
