{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chap 2.6: Feature engineering\n",
    "\n",
    "\n",
    "The process of feature engineering is of manipulating, transforming, selecting raw data into features that can be used in statistical analysis of prediction.\n",
    "\n",
    "* statistical features\n",
    "* temporal features\n",
    "* spectral features (Fourier and Wavelet transforms)\n",
    "\n",
    "We will first calculate some of the features individually on a seismic data set, then we will use a python toolbox to calculate them automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules for seismic data and feature extraction\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.signal as signal\n",
    "\n",
    "# time series feature extraction python toolbox:\n",
    "import tsfresh\n",
    "\n",
    "# seismic python toolbox\n",
    "import obspy\n",
    "import obspy.clients.fdsn.client as fdsn\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download seismic data from Puget Sound for a large M8.2 earthquake that happened in Alaska, July 29, 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download seismic data\n",
    "network = 'UW'\n",
    "station = 'RATT'\n",
    "channel = 'HHZ'# this channel gives a low frequency, 1Hz signal.\n",
    "Tstart = UTCDateTime(2021,7,29,6,15)\n",
    "Tend = Tstart+7200# UTCDateTime(year=2022, month=10, day=8)\n",
    "fdsn_client = fdsn.Client('IRIS')\n",
    "Z = fdsn_client.get_waveforms(network=network, station=station, location='--', channel=channel, starttime=Tstart, \\\n",
    "    endtime=Tend, attach_response=True)\n",
    "Z.merge(); Z.detrend(type='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the time series\n",
    "T = np.linspace(0,(Tend-Tstart),Z[0].stats.npts)\n",
    "plt.plot(T,Z[0].data)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Time since origin (s)')\n",
    "plt.title('M8.2 2021/07/29 recorded at UW.RATT.HHZ')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, only seismologists work on obspy stream, so we will convert the data into numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.asarray(Z[0].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the statistical features of the data, we will first look at the data distribution, P(z):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.hist(z,100);plt.grid(True);plt.xlabel('ground motion');plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculated the distribution of the data, ``p``, in 100 bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6.2 Statistical Features\n",
    "\n",
    "Let be $P(z)$ the distribution of the data $z$.\n",
    "\n",
    "### The mean\n",
    "<div>\n",
    "<img src=\"mean.png\" alt=\"mean\" height=300  />\n",
    "</div>\n",
    "\n",
    "Image taken from this [blog](!https://gregorygundersen.com/blog/2020/04/11/moments).\n",
    "\n",
    "The mean is the sum of the values divided by the number of data points. It is the first raw moment of a distribution. \n",
    "$\\mu = \\int_{-\\infty}^\\infty zP(z)dz$, where z the ground motion value (bin) and $P(z)$ is the distribution of the data.\n",
    "\n",
    "### The Variance\n",
    " <div>\n",
    "<img src=\"variance.png\" alt=\"variance\" height=200  />\n",
    "</div>\n",
    "\n",
    "The variance is the second *centralized* moment. *Centralized* means that the distribution is shifted around the mean. It calculates how spread out is a distribution.\n",
    "\n",
    "$\\sigma^2 = \\int_{-\\infty}^\\infty (z-\\mu)^2P(z)dz$\n",
    "\n",
    "The standard deviation is the square root of the variance, $\\sigma$. A high variance indicates a wide distribution.\n",
    "\n",
    "### The skewness\n",
    "\n",
    "Skewness is the third *standardized* moment. The *standardized* moment is scaled by the standard deviation. It measures the relative size of the two tails of the distribution.\n",
    "\n",
    "\n",
    "$m_3= \\int_{-\\infty}^\\infty \\frac{(z - \\mu)^3}{\\sigma^3}P(z)dz$\n",
    "\n",
    "With the cubic exponent, it is possible that the skewness is negative.\n",
    "\n",
    " <div>\n",
    "<img src=\"skewness.png\" alt=\"skewness\" height=200  />\n",
    "</div>\n",
    "\n",
    "Image taken from this [blog](!https://gregorygundersen.com/blog/2020/04/11/moments).\n",
    "\n",
    "A positively skewed distribution is one where most of the weight is at the end of the distribution. A negatively skewed distribution is one where most of the weight is at the beginning of the distribution.\n",
    "\n",
    "\n",
    "### Kurtosis\n",
    "\n",
    "Kurtosis measures the combine size of the two tails relative to the whole distribution. It is the fourth centralized and standardized moment.\n",
    "\n",
    "$m_4= \\int_{-\\infty}^\\infty (\\frac{z-\\mu}{\\sigma})^4P(z)dz$\n",
    "\n",
    " <div>\n",
    "<img src=\"kurtosis.png\" alt=\"kurtosis\" height=200  />\n",
    "</div>\n",
    "The laplace, normal, and uniform distributions have a mean of 0 and a variance of 1. But their kurtosis is 3, 0, and -1.2.\n",
    "\n",
    "\n",
    "Python functions to calculate the moments might be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_moment(X, k, c=0):\n",
    "    return ((X - c)**k).mean()\n",
    "\n",
    "def central_moment(X, k):\n",
    "    return raw_moment(X=X, k=k, c=X.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now calculate the mean, variance, skewness, and kurtosis of the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter answers here using the functions for the moment.\n",
    "# the mean:\n",
    "print(raw_moment(z,1))\n",
    "\n",
    "# the variance:\n",
    "print(central_moment(z,2))\n",
    "\n",
    "# the skewness\n",
    "print(central_moment(z,3)/central_moment(z,2)**(3/2))\n",
    "\n",
    "# the kurtosis\n",
    "print(central_moment(z,4)/central_moment(z,2)**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the numpy and scipy modules to get these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the mean is %4.2f, the variance is %4.2f, the skewness is %4.2f, the kurtosis is %4.2f'\n",
    " %(np.mean(z),np.std(z)**2,scipy.stats.skew(z),scipy.stats.kurtosis(z,fisher=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values may mean nothing without some additional context. We can download seismic noise data to see if the earthquake waveforms is statistically different from the noise. For that, we will download the same length of data prior to the earthquake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download seismic data 2 hours prior to the earthquake.\n",
    "# modify below.\n",
    "N = fdsn_client.get_waveforms(network=network, station=station, location='--', channel=channel, starttime=Tstart-7200, \\\n",
    "    endtime=Tstart, attach_response=True)\n",
    "N.merge(); N.detrend(type='linear')\n",
    "n=np.asarray(N[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "T = np.linspace(0,(Tend-Tstart),N[0].stats.npts)\n",
    "plt.plot(T,n)\n",
    "plt.grid(True)\n",
    "plt.xlabel('2 hours before earthquake origin')\n",
    "plt.title('Noise recorded at UW.RATT.HHZ')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and plot the distribution of ground motion noise values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type below\n",
    "pn=plt.hist(n,100,alpha=0.5,label='noise');plt.grid(True);plt.xlabel('ground motion');plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the mean is %4.2f, the variance is %4.2f, the skewness is %4.2f, the kurtosis is %4.2f'\n",
    " %(np.mean(n),np.std(n)**2,scipy.stats.skew(n),scipy.stats.kurtosis(n,fisher=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the values. You notice that for both quake and noise, the mean is small compared to absolute values; skewness is small (it's mostly symmetric). But the variance and the kurtosis are high for earthquakes. In fact, these two are used to detect earthquakes.\n",
    "\n",
    "\n",
    "## 2.6.3 temporal features\n",
    "\n",
    "These are calculated based on the time series, such as: absolute energy, autocorrelation, centroid, entropy, zero crossing rate etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6.4 Spectral features\n",
    "\n",
    "The data can be projected onto series of orthogonal basis to represent the multiple scale of the data. The **Fourier transform** is the most utilized data transform to explore the frequencies that compose the signal.\n",
    "\n",
    "\n",
    "### Fourier Transform\n",
    "We use the Scipy Fourier [package](!https://docs.scipy.org/doc/scipy/reference/generated/scipy.fftpack.rfft.html#scipy.fftpack.rfft) to transform the two time series (earthquake and noise).\n",
    "\n",
    "The Fourier transform is a decomposition of the time series onto a orthonormal basis of cosine and sine functions. The Fourier transform of a time series $f(t)$ (but similarly if the variable is space $x$).\n",
    "\n",
    "$\\hat{F}(f) = \\int_{-\\infty}^\\infty f(t) \\exp^{-i2\\pi ft} dt$\n",
    "\n",
    "$\\hat{F}(f)$ is the complex Fourier value at frequency $f$. The FOurier transform determines what frequency(ies) dominate the time series.\n",
    "\n",
    "\n",
    "### Nyquist\n",
    "The Fourier transform we will use in this class takes a discrete time series of real numbers. The time series is sampled with $N$ samples per seconds. If the time series span $T$ seconds regularly, then the sampling rate of the data $dt=T/N$. The highest frequency that can be resolved in a discrete time series, called the Nyquist frequency, is limited by $dt$:\n",
    "\n",
    "$F_{Nyq} = \\frac{1}{2dt N}$\n",
    "\n",
    "Effectively, one cannot constrain signals within two time samples from the data.\n",
    "\n",
    "### Uncertainties\n",
    "* The discrete Fourier Transform yields an approximation of the FT. The shorter the time series, the least accurate is the FT. This means that the FT on short time windows is less accurate\n",
    "* The FT assumes (and requires) the periodicity of the series, meaning that the finite/trimmed time series would repeat in time. To enforce this, we  **taper** the time series so that the first and last points are equal (to zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft, ifft, fftfreq, next_fast_len\n",
    "\n",
    "npts = Z[0].stats.npts\n",
    "## FFT the signals\n",
    "# fill up until 2^N value to speed up the FFT\n",
    "Nfft = next_fast_len(int(Z[0].data.shape[0])) # this will be an even number\n",
    "freqVec = fftfreq(Nfft, d=Z[0].stats.delta)[:Nfft//2]\n",
    "Z.taper(max_percentage=0.05)\n",
    "Zhat = fft(Z[0].data,n=Nfft)#/np.sqrt(Z[0].stats.npts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the Obspy documentation to find out about the [taper](!https://docs.obspy.org/master/packages/autogen/obspy.core.trace.Trace.taper.html#supported-methods) function.\n",
    "Plot the amplitude and phase spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,1,figsize=(11,8))\n",
    "ax[0].plot(freqVec,np.abs(Zhat[:Nfft//2])/Nfft)\n",
    "ax[0].grid(True)\n",
    "ax[0].set_xscale('log');ax[0].set_yscale('log')\n",
    "ax[0].set_xlabel('Frequency (Hz)');ax[0].set_ylabel('Amplitude (m/s)')\n",
    "ax[1].hist(np.angle(Zhat))\n",
    "ax[1].grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will note above that the phase values are randomly distributed between -pi and pi. We can check it by showing the distribution of the phase and amplitude spectra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your turn. Plot the histograms of the phase and amplitude spectrum\n",
    "plt.hist(np.log10(np.abs(Zhat[:Nfft//2])/Nfft),100);plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also analyze the spectral characteristics of the noise time series. Below, \n",
    "1. compute the Fourier transform\n",
    "2. plot the phase and amplitude spectra\n",
    "3. plot the distribution of the phase and amplitude values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Fourier transform of the noise time series\n",
    "\n",
    "npts1 = N[0].stats.npts\n",
    "## FFT the signals\n",
    "# fill up until 2^N value to speed up the FFT\n",
    "Nfft1 = next_fast_len(int(N[0].data.shape[0])) # this will be an even number\n",
    "freqVec1 = fftfreq(Nfft1, d=N[0].stats.delta)[:Nfft1//2]\n",
    "\n",
    "# taper the data to enable periodicity\n",
    "N.taper(max_percentage=0.05)\n",
    "\n",
    "# Fourier transform\n",
    "Nhat = fft(N[0].data,n=Nfft1)#/np.sqrt(Z[0].stats.npts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the phase and amplitude spectra\n",
    "fig,ax=plt.subplots(2,1,figsize=(11,8))\n",
    "ax[0].plot(freqVec1,np.abs(Nhat[:Nfft1//2])/Nfft1)\n",
    "ax[0].grid(True)\n",
    "ax[0].set_xscale('log');ax[0].set_yscale('log')\n",
    "ax[0].set_xlabel('Frequency (Hz)');ax[0].set_ylabel('Amplitude (m/s)')\n",
    "ax[1].hist(np.angle(Nhat))\n",
    "ax[1].grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay the spectrum of the data and the spectrum of the noise\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(11,8))\n",
    "ax.plot(freqVec,np.abs(Zhat[:Nfft//2])/Nfft)\n",
    "ax.plot(freqVec1,np.abs(Nhat[:Nfft1//2])/Nfft1)\n",
    "ax.grid(True)\n",
    "ax.set_xscale('log');ax.set_yscale('log')\n",
    "ax.set_xlabel('Frequency (Hz)');ax.set_ylabel('Amplitude (m/s)')\n",
    "ax.legend(['Earthquake','Noise'])\n",
    "ax.set_ylim([1e-5,1e4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering in the Fourier space\n",
    "\n",
    "The data may super-impose multiple signals of various frequencies. To remove or extract specific signals that do not overlap in frequencies, we can *filter* the data.\n",
    "\n",
    "We will use the ``scipy.signal`` module to filter the earthquake time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos = signal.butter(10, 15, 'hp', fs=fs, output='sos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-Time Fourier Transform (SFTF)\n",
    "\n",
    "In time-dependent and multi-scale problems, it may be interesting to extract data features from the short time Fourier transform.\n",
    "\n",
    "The STFT is a Fourier Transform applied to short (overlapping) windows to resolve the frequencies over different times in the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import stft\n",
    "\n",
    "nperseg=1000\n",
    "z=np.asarray(Z[0].data)\n",
    "f, t, Zxx = stft(z, fs=100, nperseg=nperseg,noverlap=200)\n",
    "print(np.max(np.max(np.abs(Zxx))))\n",
    "fig,ax=plt.subplots(2,1,figsize=(11,8),sharex=True)\n",
    "ax[0].pcolormesh(t/3600, f, np.log10(np.abs(Zxx)), vmin=-1, vmax=3.5, shading='gouraud')\n",
    "ax[0].set_title('STFT Magnitude')\n",
    "ax[0].set_ylabel('Frequency [Hz]')\n",
    "# ax[0].set_xlabel('Time [Hours]')\n",
    "ax[0].set_yscale('log');ax[0].set_ylim(0.1,40)\n",
    "\n",
    "\n",
    "n=np.asarray(N[0].data)\n",
    "f, t, Zxx = stft(n, fs=100, nperseg=nperseg,noverlap=200)\n",
    "print(np.max(np.max(np.abs(Zxx))))\n",
    "ax[1].pcolormesh(t/3600, f, np.log10(np.abs(Zxx)), vmin=-1, vmax=0.5, shading='gouraud')\n",
    "# ax[1].set_title('Noise Magnitude')\n",
    "ax[1].set_ylabel('Frequency [Hz]')\n",
    "ax[1].set_xlabel('Time [Hours]');ax[1].set_yscale('log');ax[1].set_ylim(0.1,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavelet transform\n",
    "\n",
    "The wavelet transform projects the time series onto a 2D space of *time* and *scale* axis. The *scale* is a representation of the frequency scale of the data. The waveletn transforms uses a series of functions called *wavelets* to linearly decompose the signals. Unlike the *sine* functions of the Fourier transform, the *wavelets* have finite durations and are localized in time:\n",
    "\n",
    " <div>\n",
    "<img src=\"Wavelet-Out1.jpeg\" alt=\"wavelet\" height=100  />\n",
    "</div>\n",
    "\n",
    "Image from this [article](!https://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/).\n",
    "\n",
    "There exist many canonical wavelet families. The difference between families are typically their shape, compactness, and smoothness. Typically, one chooses one family for the specific time series. Wavelets have finite energy and zero mean.\n",
    "\n",
    "The time-scale representation of of a time series is a *scaleogram*. *scales* can be converted to pseudo frequencies: If $f_c$ is the central frequency of the wavelet and the scale is $a$, then the pseuo-frequency is $f_a = f_c/a$.\n",
    "\n",
    "\n",
    " <div>\n",
    "<img src=\"wavelet_families.png\" alt=\"wavelet\" height=300  />\n",
    "</div>\n",
    "\n",
    "Image from this [article](!https://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/).\n",
    "\n",
    "The wavelet transform becomes:\n",
    "\n",
    "$\\hat{F}(a,b) = \\frac{1}{\\sqrt{a}} \\int_{-\\infty}^\\infty f(t) \\bar{\\Psi} (\\frac{t-b}{a}) dt$\n",
    "\n",
    "where $\\bar{\\Psi}$ is the mother wavelet scaled by a factor of $a$ and translated/shifted by $b$. The wavelet transform is scaled by the continuous and \"infinite number of values\" of $a$ and $b$ are continuous. The *Discrete Wavelet Transform* is the wavelet transform performed on a finite number of scales and shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as signal\n",
    "\n",
    "\n",
    "t = np.arange(0,(Tend-Tstart)+Z[0].stats.delta,Z[0].stats.delta)\n",
    "fs=1/Z[0].stats.delta\n",
    "\n",
    "# use the number of scales\n",
    "w = 6.\n",
    "\n",
    "# relate scales with frequencies\n",
    "# freq = np.linspace(0, fs/2, 100)\n",
    "freq = np.logspace(-1, np.log10(fs/2), 100)\n",
    "widths = w*fs / (2*freq*np.pi)\n",
    "\n",
    "\n",
    "cwtm = signal.cwt(z, signal.morlet2, widths, w=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the time-frequency representation of large time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cwtmatr = signal.cwt(z, signal.morlet, widths)\n",
    "plt.imshow(np.log10(np.abs(cwtm)), extent=[t.min(),t.max(),freq.min(),freq.max()], cmap='viridis', aspect='auto',\n",
    "           vmax=5, vmin=-0.5,origin='lower')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF transforms take computational time in the workflow, let's compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit cwtm = signal.cwt(z, signal.morlet2, widths, w=w)\n",
    "%timeit f, t, Zxx = stft(n, fs=100, nperseg=nperseg,noverlap=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these transform, we can extract similar statistical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Feature Extraction\n",
    "\n",
    "There are toolboxes to extract features from the data, including feautres done the tranfsform. We will use ``Tsfresh``, some of the best maintained and comprehensive python toolbox.\n",
    "\n",
    "Most toolboxes require the input to be ``pandas``. First, we convert the data into pandas.  We need to [format the data](!https://tsfresh.readthedocs.io/en/latest/text/data_formats.html) for input into tsfresh. It needs 1 column with the ``id`` (or label), one column for the time stamps (``sort``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's put the data into the data frame. n is the noise time series, z is the quake time series, t is the vector of time.\n",
    "D = {'id':\"quake\",'time':t[0:len(n)],'w':z[:len(n)] }\n",
    "D1 =  {'id':\"noise\",'time':t[0:len(n)],'w':n[:len(n)] }\n",
    "\n",
    "df=pd.DataFrame.from_dict(D)\n",
    "df=pd.concat([df,pd.DataFrame.from_dict(D1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[df['id']=='noise']['w'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tsfresh can calculate around 1200 features. This is particularly computationally intensive. Instead, we will use the ``MinimalFCParameters`` set of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import  MinimalFCParameters\n",
    "settings = MinimalFCParameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features = extract_features(df, column_id='id',column_sort=\"time\",column_value=\"w\",default_fc_parameters=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features_all = extract_features(df, column_id='id',column_sort=\"time\",column_value=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In practice, running TF extraction over continuous data\n",
    "\n",
    "We will test if these features can help us discriminate between noise and earthquakes. We will slide through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download seismic data\n",
    "network = 'UW'\n",
    "station = 'RATT'\n",
    "channel = 'HHZ'# this channel gives a low frequency, 1Hz signal.\n",
    "Tstart = UTCDateTime(2021,7,29,6,15)-7200\n",
    "Tend = Tstart+7200*2# UTCDateTime(year=2022, month=10, day=8)\n",
    "fdsn_client = fdsn.Client('IRIS')\n",
    "A = fdsn_client.get_waveforms(network=network, station=station, location='--', channel=channel, starttime=Tstart, \\\n",
    "    endtime=Tend, attach_response=True)\n",
    "A.merge(); A.detrend(type='linear');A.taper(max_percentage=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will trim the trace to form short, 10s window, using the obspy function ``trim``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=int(N[0].stats.sampling_rate)\n",
    "AA=np.reshape(A[0].data[:-1],(int(10*fs),(len(A[0].data)-1)//int(10*fs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AA.shape[0])\n",
    "print(np.arange(0,10.,1/fs).shape)\n",
    "print(AA[:,1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put that data into a pandas data frame. We do not have labels, we will use the index of the window as ``id``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for i in range(0,AA.shape[1]):\n",
    "    D =  {'id':i*np.ones(AA.shape[0]),'time':np.arange(0,10.,1/fs),'w':AA[:,i] }\n",
    "    if len(df)==0:\n",
    "        df=pd.DataFrame.from_dict(D)\n",
    "    elif len(df)>0:\n",
    "        df=pd.concat([df,pd.DataFrame.from_dict(D)])\n",
    "        \n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_continuous = extract_features(df, column_id='id',column_sort=\"time\",column_value=\"w\",default_fc_parameters=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_continuous.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_continuous.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlgeo_sk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6825af05e3ea1f79f5651bdd3095330bdaee3a1e3958825bcb0ebbb42c21bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
